{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessar's Stable Diffusion Notebook V0.2\n",
    "\n",
    "Copy of [Stable Diffusion](https://colab.research.google.com/github/pharmapsychotic/ai-notebooks/blob/main/pharmapsychotic_Stable_Diffusion.ipynb) notebook by [@pharmapsychotic](https://twitter.com/pharmapsychotic) \n",
    "\n",
    "This lets you generate images with CompVis/Stability [Stable Diffusion](https://github.com/CompVis/stable-diffusion) with bonus [KLMS sampling](https://github.com/crowsonkb/k-diffusion.git) from [@RiversHaveWings](https://twitter.com/RiversHaveWings)\n",
    "\n",
    "You need to get the ckpt file and put it on your Google Drive first to use this. It can be downloaded from [HuggingFace](https://huggingface.co/CompVis/stable-diffusion).\n",
    "\n",
    "## Changelog\n",
    "V0.2\n",
    "* Add freeze_seed option\n",
    "* Add guidance_scale_step option\n",
    "\n",
    "V0.1\n",
    "* Added option to reroll with random seed\n",
    "* Added seed and step value to output name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Mount Google Drive and Prepare Folders\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "outputs_path = \"/content/gdrive/MyDrive/AI/StableDiffusion\"\n",
    "!mkdir -p $outputs_path\n",
    "print(f\"Outputs will be saved to {outputs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installation\n",
    "!pip install pytorch-lightning torch-fidelity\n",
    "!pip install numpy omegaconf einops kornia pytorch-lightning\n",
    "!pip install albumentations transformers\n",
    "!pip install ftfy jsonmerge resize-right torchdiffeq tqdm\n",
    "\n",
    "!git clone https://github.com/CompVis/stable-diffusion\n",
    "%cd stable-diffusion/\n",
    "!git clone https://github.com/CompVis/taming-transformers\n",
    "!git clone https://github.com/openai/CLIP.git\n",
    "!git clone https://github.com/crowsonkb/k-diffusion.git\n",
    "\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"./CLIP\")\n",
    "sys.path.append('./taming-transformers')\n",
    "sys.path.append('./k-diffusion')\n",
    "\n",
    "!echo '' > ./k-diffusion/k_diffusion/__init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown You need to get the model weights yourself and put on Google Drive or this Colab instance\n",
    "checkpoint_model_file = \"/content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup\n",
    "\n",
    "import argparse, gc, json, os, random, sys, time, glob, requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import PIL\n",
    "from contextlib import contextmanager, nullcontext\n",
    "from einops import rearrange, repeat\n",
    "from IPython.display import display, clear_output\n",
    "from itertools import islice\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.cuda.amp import autocast\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "from k_diffusion.sampling import sample_lms\n",
    "from k_diffusion.external import CompVisDenoiser\n",
    "\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "class CFGDenoiser(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.inner_model = model\n",
    "\n",
    "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
    "        x_in = torch.cat([x] * 2)\n",
    "        sigma_in = torch.cat([sigma] * 2)\n",
    "        cond_in = torch.cat([uncond, cond])\n",
    "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
    "        return uncond + (cond - uncond) * cond_scale\n",
    "\n",
    "class config():\n",
    "    def __init__(self):\n",
    "        self.ckpt = checkpoint_model_file\n",
    "        self.config = 'configs/stable-diffusion/v1-inference.yaml'\n",
    "        self.ddim_eta = 0.0\n",
    "        self.ddim_steps = 100\n",
    "        self.fixed_code = True\n",
    "        self.init_img = None\n",
    "        self.n_iter = 1\n",
    "        self.n_samples = 1\n",
    "        self.outdir = \"\"\n",
    "        self.precision = 'full' # 'autocast'\n",
    "        self.prompt = \"\"\n",
    "        self.sampler = 'klms'\n",
    "        self.scale = 7.5\n",
    "        self.seed = 42\n",
    "        self.strength = 0.75 # strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
    "        self.H = 512\n",
    "        self.W = 512\n",
    "        self.C = 4\n",
    "        self.f = 8\n",
    "      \n",
    "def load_img(path, w, h):\n",
    "    if path.startswith('http://') or path.startswith('https://'):\n",
    "        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n",
    "    else:\n",
    "        if os.path.isdir(path):\n",
    "            files = [file for file in os.listdir(path) if file.endswith('.png') or file .endswith('.jpg')]\n",
    "            path = os.path.join(path, random.choice(files))\n",
    "            print(f\"Chose random init image {path}\")\n",
    "        image = Image.open(path).convert('RGB')\n",
    "    image = image.resize((w, h), Image.LANCZOS)\n",
    "    w, h = image.size\n",
    "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.*image - 1.\n",
    "\n",
    "opt = config()\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "batch_idx = 0\n",
    "sample_idx = 0\n",
    "\n",
    "def generate(opt):\n",
    "    global sample_idx\n",
    "    seed_everything(opt.seed)\n",
    "    os.makedirs(opt.outdir, exist_ok=True)\n",
    "\n",
    "    if opt.sampler == 'plms':\n",
    "        sampler = PLMSSampler(model)\n",
    "    else:\n",
    "        sampler = DDIMSampler(model)\n",
    "\n",
    "    model_wrap = CompVisDenoiser(model)       \n",
    "    batch_size = opt.n_samples\n",
    "    prompt = opt.prompt\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "    init_latent = None\n",
    "\n",
    "    if opt.init_img != None and opt.init_img != '':\n",
    "        init_image = load_img(opt.init_img, opt.W, opt.H).to(device)\n",
    "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
    "\n",
    "    t_enc = int(opt.strength * opt.ddim_steps)\n",
    "\n",
    "    start_code = None\n",
    "    if opt.fixed_code and init_latent == None:\n",
    "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
    "\n",
    "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
    "\n",
    "    images = []\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                for n in range(opt.n_iter):\n",
    "                    for prompts in data:\n",
    "                        uc = None\n",
    "                        if opt.scale != 1.0:\n",
    "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                        if isinstance(prompts, tuple):\n",
    "                            prompts = list(prompts)\n",
    "                        c = model.get_learned_conditioning(prompts)\n",
    "\n",
    "                        if init_latent != None:\n",
    "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                            samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
    "                                                    unconditional_conditioning=uc,)\n",
    "                        else:\n",
    "\n",
    "                            if opt.sampler == 'klms':\n",
    "                                print(\"Using KLMS sampling\")\n",
    "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "                                sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
    "                                model_wrap_cfg = CFGDenoiser(model_wrap)\n",
    "                                x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0]\n",
    "                                extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
    "                                samples = sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
    "                            else:\n",
    "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "                                samples, _ = sampler.sample(S=opt.ddim_steps,\n",
    "                                                                conditioning=c,\n",
    "                                                                batch_size=opt.n_samples,\n",
    "                                                                shape=shape,\n",
    "                                                                verbose=False,\n",
    "                                                                unconditional_guidance_scale=opt.scale,\n",
    "                                                                unconditional_conditioning=uc,\n",
    "                                                                eta=opt.ddim_eta,\n",
    "                                                                x_T=start_code)\n",
    "\n",
    "                        x_samples = model.decode_first_stage(samples)\n",
    "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                        for x_sample in x_samples:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            images.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
    "                            filepath = os.path.join(opt.outdir, f\"{batch_name}({batch_idx})_{sample_idx:04}_seed_{opt.seed}_scale_{opt.scale}.png\")\n",
    "                            print(f\"Saving to {filepath}\")\n",
    "                            Image.fromarray(x_sample.astype(np.uint8)).save(filepath)\n",
    "                            sample_idx += 1\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Image creation\n",
    "\n",
    "#@markdown `batch_name`: name for subfolder and filenames<br>\n",
    "#@markdown `width_height`: image dimensions<br>\n",
    "#@markdown `guidance_scale`: strength of text prompt<br>\n",
    "#@markdown `guidance_scale_step`: amount to increase/decrease scale with each iteration<br>\n",
    "#@markdown `steps`: number of diffusion steps<br>\n",
    "#@markdown `num_batch_images`: how many images you want to generate in this batch<br>\n",
    "#@markdown `sampler`: KLMS is recommended<br>\n",
    "#@markdown `ddim_eta`: scale of variance from 0.0 to 1.0<br>\n",
    "#@markdown `seed`: use -1 for random seed or specify number manually<br>\n",
    "#@markdown `freeze_seed`: keep the same seed between iterations (useful together with guidance_scale_step)<br>\n",
    "#@markdown `random_seed_reroll`: if you want a random seed everytime. If false, it will increment initial seed by 1<br>\n",
    "#@markdown `init_image_or_folder`: url or path to an image, or path to a folder to pick random images from<br>\n",
    "#@markdown `init_strength`: from 0.0 to 1.0 how much the init image is used<br>\n",
    "\n",
    "#@markdown \n",
    "\n",
    "#@markdown Batch settings\n",
    "batch_name = \"20220823_purple_panda\" #@param {type:\"string\"}\n",
    "width_height = [512, 512] #@param{type: 'raw'}\n",
    "guidance_scale = 8.0 #@param {type:\"number\"}\n",
    "guidance_scale_step = 0 #@param {type:\"number\"}\n",
    "steps = 50 #@param {type:\"integer\"}\n",
    "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
    "num_batch_images = 10 #@param {type:\"integer\"}\n",
    "sampler = 'klms' #@param [\"klms\",\"plms\", \"ddim\"]\n",
    "ddim_eta = 0.75 #@param {type:\"number\"}\n",
    "seed = -1 #@param {type:\"integer\"}\n",
    "freeze_seed = False #@param {type: \"boolean\"}\n",
    "random_seed_reroll = True #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown \n",
    "\n",
    "#@markdown Init image\n",
    "\n",
    "\n",
    "init_image_or_folder = \"\" #@param {type:\"string\"}\n",
    "init_strength = 0 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown \n",
    "\n",
    "#@markdown Prompt\n",
    "prompt = \"photo of apurple panda playing playstation on a couch, wide shot, realistic, natural lighting, hd\" #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "opt.init_img = init_image_or_folder\n",
    "opt.ddim_steps = steps\n",
    "opt.n_iter = 1\n",
    "opt.n_samples = samples_per_batch\n",
    "opt.outdir = os.path.join(outputs_path, batch_name)\n",
    "opt.prompt = prompt\n",
    "opt.sampler = sampler\n",
    "opt.scale = guidance_scale\n",
    "opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
    "opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
    "opt.W = width_height[0]\n",
    "opt.H = width_height[1]\n",
    "\n",
    "if opt.strength >= 1 or init_image_or_folder == None:\n",
    "    opt.init_img = \"\"\n",
    "\n",
    "if opt.init_img != None and opt.init_img != '':\n",
    "    opt.sampler = 'ddim'\n",
    "\n",
    "if opt.sampler != 'ddim':\n",
    "    opt.ddim_eta = 0.0\n",
    "\n",
    "# save settings\n",
    "settings = {\n",
    "    'ddim_eta': ddim_eta,\n",
    "    'guidance_scale': guidance_scale,\n",
    "    'guidance_scale_step': guidance_scale_step,\n",
    "    'init_image': init_image_or_folder,\n",
    "    'init_strength': init_strength,\n",
    "    'num_batch_images': num_batch_images,\n",
    "    'prompt': prompt,\n",
    "    'sampler': sampler,\n",
    "    'samples_per_batch': samples_per_batch,\n",
    "    'seed': opt.seed,\n",
    "    'freeze_seed': freeze_seed,\n",
    "    'random_seed_reroll': random_seed_reroll,\n",
    "    'steps': steps,\n",
    "    'width': opt.W,\n",
    "    'height': opt.H,\n",
    "}\n",
    "os.makedirs(opt.outdir, exist_ok=True)\n",
    "while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\"):\n",
    "    batch_idx += 1\n",
    "with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
    "    json.dump(settings, f, ensure_ascii=False, indent=4)\n",
    "sample_idx = 0\n",
    "\n",
    "for i in range(num_batch_images):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    images = generate(opt)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Used seed: {opt.seed}\")\n",
    "    print(f\"Saved to: {opt.outdir}\")\n",
    "    for image in images:\n",
    "        display(image)\n",
    "\n",
    "    if freeze_seed == False:\n",
    "        opt.seed = random.randint(0, 2**32) if random_seed_reroll == True else opt.seed + 1\n",
    "    opt.scale = opt.scale + guidance_scale_step\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
