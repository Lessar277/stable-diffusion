{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lessar277/stable-diffusion-colab/blob/main/pharmapsychotic_Stable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU52ZvES6-1T"
      },
      "source": [
        "# Lessar's Stable Diffusion Notebook V0.6\n",
        "\n",
        "Copy of [Stable Diffusion](https://colab.research.google.com/github/pharmapsychotic/ai-notebooks/blob/main/pharmapsychotic_Stable_Diffusion.ipynb) notebook by [@pharmapsychotic](https://twitter.com/pharmapsychotic) \n",
        "\n",
        "This lets you generate images with CompVis/Stability [Stable Diffusion](https://github.com/CompVis/stable-diffusion) with bonus [KLMS sampling](https://github.com/crowsonkb/k-diffusion.git) from [@RiversHaveWings](https://twitter.com/RiversHaveWings)\n",
        "\n",
        "You need to get the ckpt file and put it on your Google Drive first to use this. It can be downloaded from [HuggingFace](https://huggingface.co/CompVis/stable-diffusion).\n",
        "\n",
        "## Changelog\n",
        "V0.6\n",
        "* Fixed `ddim_eta` actually being used (from pharmapsychotic's notebook)\n",
        "* Fixed filenames being saved with the wrong sampler when `iterate_samplers` was on\n",
        "* Simplified seed behaviour, there is now a dropdown where you can choose between random, static or iterative seeds\n",
        "* Added a workaround for the script crashing if steps are set between 101 and 119. If steps are set to a value within this range (either manually or by using `step_increase`), the script sets the steps to 120 automatically \n",
        "\n",
        "V0.5\n",
        "* Added dpm2, dpm2_a, heun, euler, euler_a samplers\n",
        "* added the ability to increase step count with each iteration\n",
        "* added the option to iterate through samplers (basic)\n",
        "* updated layout (scale step, step increase and sampler iteration now have their own experiments section) \n",
        "\n",
        "V0.4\n",
        "* Added `sample_loopback` option\n",
        "\n",
        "V0.3\n",
        "* Updated to latest pharmapsychotic version, to include mixed precision\n",
        "\n",
        "V0.2\n",
        "* Add freeze_seed option\n",
        "* Add guidance_scale_step option\n",
        "\n",
        "V0.1\n",
        "* Added option to reroll with random seed\n",
        "* Added seed and step value to output name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uc5OwvKdjRJF"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DeqQ7pt1zdI7"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive and Prepare Folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/StableDiffusion\"\n",
        "!mkdir -p $outputs_path\n",
        "print(f\"Outputs will be saved to {outputs_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installation\n",
        "!pip install pytorch-lightning torch-fidelity\n",
        "!pip install numpy omegaconf einops kornia pytorch-lightning\n",
        "!pip install albumentations transformers\n",
        "!pip install ftfy jsonmerge resize-right torchdiffeq tqdm\n",
        "\n",
        "!git clone https://github.com/CompVis/stable-diffusion\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git\n",
        "\n",
        "!echo '' > ./k-diffusion/k_diffusion/__init__.py\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZCiglGzMaFfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bcHsbr3hblrk"
      },
      "outputs": [],
      "source": [
        "#@title Load Model\n",
        "\n",
        "#@markdown You need to get the model weights yourself and put on Google Drive or this Colab instance\n",
        "checkpoint_model_file = \"/content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\" #@param {type:\"string\"}\n",
        "\n",
        "import argparse, gc, json, os, random, sys, time, glob, requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import PIL\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from einops import rearrange, repeat\n",
        "from IPython.display import display, clear_output\n",
        "from itertools import islice\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append('./k-diffusion')\n",
        "sys.path.append('./stable-diffusion')\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "from k_diffusion.sampling import sample_lms, sample_euler, sample_euler_ancestral, sample_dpm_2, sample_dpm_2_ancestral, sample_heun\n",
        "from k_diffusion.external import CompVisDenoiser\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model = model.half().to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "class config():\n",
        "    def __init__(self):\n",
        "        self.ckpt = checkpoint_model_file\n",
        "        self.config = 'stable-diffusion/configs/stable-diffusion/v1-inference.yaml'\n",
        "        self.ddim_eta = 0.0\n",
        "        self.ddim_steps = 100\n",
        "        self.fixed_code = True\n",
        "        self.init_img = None\n",
        "        self.n_iter = 1\n",
        "        self.n_samples = 1\n",
        "        self.outdir = \"\"\n",
        "        self.precision = 'autocast'\n",
        "        self.prompt = \"\"\n",
        "        self.sampler = 'klms'\n",
        "        self.scale = 7.5\n",
        "        self.seed = 42\n",
        "        self.strength = 0.75 # strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
        "        self.H = 512\n",
        "        self.W = 512\n",
        "        self.C = 4\n",
        "        self.f = 8\n",
        "      \n",
        "def load_img(path, shape):\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n",
        "    else:\n",
        "        if os.path.isdir(path):\n",
        "            files = [file for file in os.listdir(path) if file.endswith('.png') or file .endswith('.jpg')]\n",
        "            path = os.path.join(path, random.choice(files))\n",
        "            print(f\"Chose random init image {path}\")\n",
        "        image = Image.open(path).convert('RGB')\n",
        "    image = image.resize(shape, resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float16) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "opt = config()\n",
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
        "model = model.to(device)\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "\n",
        "def generate(opt):\n",
        "    global sample_idx\n",
        "    seed_everything(opt.seed)\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "\n",
        "    if opt.sampler == 'plms':\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    model_wrap = CompVisDenoiser(model)       \n",
        "    batch_size = opt.n_samples\n",
        "    prompt = opt.prompt\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "    init_latent = None\n",
        "\n",
        "    if opt.init_img != None and opt.init_img != '':\n",
        "        init_image = load_img(opt.init_img, shape=(opt.W, opt.H)).to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "    t_enc = int(opt.strength * opt.ddim_steps)\n",
        "\n",
        "    start_code = None\n",
        "    if opt.fixed_code and init_latent == None:\n",
        "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    images = []\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for n in range(opt.n_iter):\n",
        "                    for prompts in data:\n",
        "                        uc = None\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        if init_latent != None:\n",
        "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                            samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                    unconditional_conditioning=uc,)\n",
        "                        else:\n",
        "\n",
        "                            if opt.sampler in [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\"]:\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
        "                                model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "                                x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0]\n",
        "                                extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
        "                                if opt.sampler == 'klms':\n",
        "                                  print(\"Using KLMS sampling\")\n",
        "                                  samples = sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                                elif opt.sampler == 'dpm2':\n",
        "                                  print(\"Using dpm2 sampling\")\n",
        "                                  samples = sample_dpm_2(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                                elif opt.sampler == 'dpm2_ancestral':\n",
        "                                  print(\"Using dpm2_ancestral sampling\")\n",
        "                                  samples = sample_dpm_2_ancestral(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                                elif opt.sampler == 'heun':\n",
        "                                  print(\"Using heun sampling\")\n",
        "                                  samples = sample_heun(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                                elif opt.sampler == 'euler':\n",
        "                                  print(\"Using euler sampling\")\n",
        "                                  samples = sample_euler(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                                elif opt.sampler == 'euler_ancestral':\n",
        "                                  print(\"Using euler_ancestral sampling\")\n",
        "                                  samples = sample_euler_ancestral(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                            else:\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                samples, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                                conditioning=c,\n",
        "                                                                batch_size=opt.n_samples,\n",
        "                                                                shape=shape,\n",
        "                                                                verbose=False,\n",
        "                                                                unconditional_guidance_scale=opt.scale,\n",
        "                                                                unconditional_conditioning=uc,\n",
        "                                                                eta=opt.ddim_eta,\n",
        "                                                                x_T=start_code)\n",
        "\n",
        "                        x_samples = model.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        for x_sample in x_samples:\n",
        "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                            images.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "                            filepath = os.path.join(opt.outdir, f\"{batch_name}({batch_idx})_{sample_idx:04}_seed_{opt.seed}_scale_{opt.scale}_sampler_{opt.sampler}.png\")\n",
        "                            print(f\"Saving to {filepath}\")\n",
        "                            Image.fromarray(x_sample.astype(np.uint8)).save(filepath)\n",
        "                            sample_idx += 1\n",
        "    return images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzmVAdZ1-5tE"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title ##Image creation\n",
        "\n",
        "#@markdown `batch_name`: name for subfolder and filenames<br>\n",
        "#@markdown `width_height`: image dimensions<br>\n",
        "#@markdown `guidance_scale`: strength of text prompt<br>\n",
        "#@markdown `steps`: number of diffusion steps<br>\n",
        "#@markdown `num_batch_images`: how many images you want to generate in this batch<br>\n",
        "#@markdown `sampler`: KLMS is recommended<br>\n",
        "#@markdown `ddim_eta`: scale of variance from 0.0 to 1.0<br>\n",
        "#@markdown `seed`: use -1 for random seed or specify number manually<br>\n",
        "#@markdown `seed_behaviour`: static keeps the same seed for each generation, iter increases seed each time by 1, random is self explanatory \n",
        "\n",
        "#@markdown ###Batch settings\n",
        "batch_name = \"20220831_Cats\" #@param {type:\"string\"}\n",
        "width_height = [512, 512] #@param{type: 'raw'}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "steps = 50 #@param {type:\"integer\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "num_batch_images = 5 #@param {type:\"integer\"}\n",
        "sampler = 'klms' #@param [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0 #@param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"integer\"}\n",
        "seed_behaviour = \"iter\" #@param [\"iter\", \"static\", \"random\"]\n",
        "\n",
        "#@markdown ###Experiments\n",
        "\n",
        "#@markdown `guidance_scale_step`: increase guidance_scale by this much for each iteration<br>\n",
        "#@markdown `step_increase`: increase steps value by this much for each iteration<br>\n",
        "#@markdown `iterate_samplers`: switch samplers with each generation. This will always start with KLMS, regardless of what sampler was chosen above. Recommended to run exactly 8 batches for this experiment. No effect if num_batch_images is greater than 8 <br>\n",
        "\n",
        "guidance_scale_step = 0 #@param {type:\"number\"}\n",
        "step_increase = 0 #@param {type:\"number\"}\n",
        "iterate_samplers = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown ###Init image\n",
        "\n",
        "#@markdown `init_image_or_folder`: url or path to an image, or path to a folder to pick random images from<br>\n",
        "#@markdown `init_strength`: from 0.0 to 1.0 how much the init image is used<br>\n",
        "#@markdown `sample_loopback`: use the generated image as `init_image` for the next iteration ⚠️ Not compatible with PLMS\n",
        "\n",
        "\n",
        "init_image_or_folder = \"\" #@param {type:\"string\"}\n",
        "init_strength = 0.3 #@param {type:\"number\"}\n",
        "sample_loopback = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown ###Prompt\n",
        "prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "opt.init_img = init_image_or_folder\n",
        "opt.sample_loopback = sample_loopback\n",
        "opt.ddim_eta = ddim_eta\n",
        "opt.ddim_steps = steps\n",
        "opt.n_iter = 1\n",
        "opt.n_samples = samples_per_batch\n",
        "opt.outdir = os.path.join(outputs_path, batch_name)\n",
        "opt.prompt = prompt\n",
        "opt.sampler = sampler\n",
        "opt.scale = guidance_scale\n",
        "opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "opt.W, opt.H = map(lambda x: x - x % 64, (width_height[0], width_height[1])) # resize to integer multiple of 64\n",
        "\n",
        "if opt.strength >= 1 or init_image_or_folder == None:\n",
        "    opt.init_img = \"\"\n",
        "\n",
        "if opt.init_img != None and opt.init_img != '':\n",
        "    opt.sampler = 'ddim'\n",
        "\n",
        "if opt.sampler != 'ddim':\n",
        "    opt.ddim_eta = 0.0\n",
        "\n",
        "# save settings\n",
        "settings = {\n",
        "    'ddim_eta': ddim_eta,\n",
        "    'guidance_scale': guidance_scale,\n",
        "    'guidance_scale_step': guidance_scale_step,\n",
        "    'step_increase': step_increase,\n",
        "    'init_image': init_image_or_folder,\n",
        "    'init_strength': init_strength,\n",
        "    'num_batch_images': num_batch_images,\n",
        "    'prompt': prompt,\n",
        "    'sampler': sampler,\n",
        "    'iterate_samplers': iterate_samplers,\n",
        "    'samples_per_batch': samples_per_batch,\n",
        "    'seed': opt.seed,\n",
        "    'seed_behaviour': seed_behaviour,\n",
        "    'steps': steps,\n",
        "    'width': opt.W,\n",
        "    'height': opt.H,\n",
        "}\n",
        "os.makedirs(opt.outdir, exist_ok=True)\n",
        "while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\"):\n",
        "    batch_idx += 1\n",
        "with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "    json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "sample_idx = 0\n",
        "\n",
        "for i in range(num_batch_images):\n",
        "    #fix buggy steps issue\n",
        "    if opt.ddim_steps >= 101 and opt.ddim_steps < 120:\n",
        "      print(\"Script is buggy with steps between 101 and 119. Setting steps to 120 to avoid crashing\")\n",
        "      opt.ddim_steps = 120\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    #sampler select\n",
        "    sampler_list = [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\",\"plms\", \"ddim\"]\n",
        "    if iterate_samplers == True and opt.n_iter <= 8:\n",
        "      opt.sampler = sampler_list[sample_idx]\n",
        "    sample_filepath = os.path.join(opt.outdir, f\"{batch_name}({batch_idx})_{sample_idx:04}_seed_{opt.seed}_scale_{opt.scale}_sampler_{opt.sampler}.png\")\n",
        "\n",
        "    #the magic\n",
        "    images = generate(opt)\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Used seed: {opt.seed}\")\n",
        "    print(f\"Saved to: {sample_filepath}\")\n",
        "    for image in images:\n",
        "        display(image)\n",
        "    \n",
        "    #seed behaviour    \n",
        "    if seed_behaviour == 'random':\n",
        "      opt.seed = random.randint(0, 2**32)\n",
        "    elif seed_behaviour == 'iter':\n",
        "      opt.seed += 1\n",
        "    \n",
        "    #experiments\n",
        "    opt.scale = opt.scale + guidance_scale_step\n",
        "    opt.ddim_steps = opt.ddim_steps + step_increase\n",
        "    if sample_loopback == True:\n",
        "      print(\"Waiting for image to be saved to googleDrive (up to 1 min)\");\n",
        "      time_to_wait = 60\n",
        "      time_counter = 0\n",
        "      while not os.path.exists(sample_filepath):\n",
        "        time.sleep(1)\n",
        "        time_counter += 1\n",
        "        if time_counter > time_to_wait:break\n",
        "      print(\"Image found\")\n",
        "      opt.init_img = sample_filepath\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Lessars_Stable_Diffusion.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}